% siminos/gudorf/thesis/chapter/gmres.tex
% $Author: predrag $ $Date: 2020-10-24 01:45:26 -0400 (Sat, 24 Oct 2020) $


Also, I still need to implement the Viswanath\rf{Visw07b} hookstep algorithm.
This ``residual vector" is sent to Arnoldi iteration in order to produce the
regular Hessenberg and Orthonormal matrices of this iterative procedure. In
between Arnoldi iterations, we check the new value of the residual, i.e.
$||b-A\delta\conf^{(k)}||$. If this meets a specific tolerance, it is then
passed to the least squares problem that defines GMRES\rf{Trefethen97}, find
y that minimizes $||H_n y - ||b|| e_1||$, this solution to the least squares
problem $y$ is then converted to Newton correction via multiplication of
orthonormal matrix $x = Q_n y$. If this Newton correction does not minimize
the residual sufficiently, GMRES is restarted. Until either a maximum
iteration number is met, or the relative differences between residuals stalls
out.

GMRES specific problems to take into consideration,
breakdown of the arnoldi iteration, if the test vector lies in the Krylov subspace,
then there will be a subdiagonal element of the
 Hessenberg matrix equal to zero (due to linear
  dependence and orthogonalization with
respect to the Krylov subspace). If this occurs then
 GMRES is restarted with another random test vector.
Numerically, this should be a condition on the value of the norm
of the vector being iterated upon.
In order to evaluate the matrix vector product $J \delta \conf$, the matrix-vector approximation previously discussed is used, where
we use $J\delta \conf \approx F(\conf +\delta \conf) - F(\conf) / \epsilon$, where $\epsilon = ||\delta \conf||$.


With what I have all reason to believe is a reasonable initial condition, a 64 by 64 discretization
of the first \ppo\ of \KS\ the initial value of the norm of the value of the mapping
$||F(u,T,L)||_2 \approx 10^-3$. I take this to be a good initial condition as the other values of norms
I have seen have had much higher initial values of this norm. The problem arises when I approximate
$J \delta \conf$. With almost any randomly generated $\delta \conf$, the norm of the vector $||J \delta \conf||_2
\approx F(u+\delta u, T + \delta T, L + \delta L) - F(u,T,L) / \epsilon $ with $||\delta \conf||_2 = \epsilon$.
yields a value that is dramatically larger than $||F(u,T,L)||_2$, and as such it does not allow for a convergent
Newton-Krylov search. The fact that the norm of the mapping of the spatiotemporal field is small, leads me to
believe that the code involving the mapping is correct, but if this is the case then the matrix-vector approximation
should be well written as well, as all it involves is this mapping function. It's this circular logic
with a hole in it that has been frustrating me to no end.

Something I can try is to use a second order approximation as opposed to a first order.
This would replace the approximate with $||J \delta \conf||_2
\approx F(u+\delta u, T + \delta T, L + \delta L) - F(u-\delta u,T - \delta T,L - \delta L) / \epsilon $


I thought it might be the form of the randomly generated perturbations that was possibly making things
go awry, as I believe it's logically justified to restrict perturbations such that the perturbed field would
yield a real field if transformed back into configuration space. I wasn't doing this before, but it seemed to
help my woes to a small extent, at least in terms of GMRES convergence; but again, in the scheme of things
it is a negligible benefit as the residual of the matrix-vector approximate far outstrips any change I
have made.

The second order finite difference approximation didn't really help the values I was getting from approximating
the \jacobianM\ so instead I tried to look for why the approximation seems so poor; I hadn't looked at the contributions
from the nonlinear and linear parts separately (I actually thought I did this a while ago but I think I had poor initial conditions
before). It turns out that the stiff components of the spatiotemporal mapping (i.e. high wavenumber modes due to the fourth power
of $q_k$) are what are contributing to the (what I consider too large) magnitude of the approximation the most.

I tried playing around with the perturbation magnitudes again, since it is mostly the high wavenumber modes contributing to the
magnitude of the approximate matrix-vector product I played around with having the perturbations to these modes be smaller in
magnitude than the rest, (i.e. some subset of the perturbations of the matrix of spatiotemporal Fourier coefficients is smaller in
magnitude than the lower (wavenumber, frequency number) modes). This didn't have enough of an effect so I instead started trying
to think of a way to justify a $2/3$ rule dealiasing (i.e. damping by setting equal to zero) of these contributions. This is
usually done during time-stepping, and is justified via an energy input argument. There is usually a contribution to the energy
of a solution due to aliasing and as such the higher modes are damped in any calculations in order to maintain energy-balance.

Also, the GMRES procedure is stalling out, so I have been looking towards ways to fix this. It seems that this is a relatively
well studied problem and the answer is to introduce preconditioning matrices into the GMRES scheme. This is done in a number
of ways and I have to find the best one for my problem, but I am also trying to find one that fits into my code already.
I think I will implement FGMRES, where the F stands for ``Flexible"\rf{Saad93}. It is called this because it
encourages the preconditioner to change at every outer-loop (Newton) step, while remaining the "same" for the inner-loop (arnoldi or GMRES)
steps. (same in quotes because at every iteration the rule to generate the preconditioner stays the same, but the matrix technically
changes in size).

I was being dumb with how I was creating the random perturbation vectors for my
GMRES procedure. After thinking about it a little more I realized I should just
create a random field of initial conditions in configuration space and then perform
a spatiotemporal Fourier transform to get the correct form of field. I can't explain
how much easier this is than what I was doing before. With this the stiff parts of the
linear portion of the mapping and as such I have returned the previous damping
of higher modes back to undamped status.
I've also been reading a bunch of
papers\rf{ChaJac84,Saad93,EldSim12,MoReHa14,LotYe05,GrGrReSz16,TBAMR92} on
preconditioning methods; and specifically for GMRES algorithm. It seems to be
a heuristic science much like taking Poincar\'e sections so I might have to
try a number of different preconditioners before I get one that works, or use
a combination of multiple different ones with different properties.

Talked to xiong to get some new ideas; He thinks that what I was trying to do
with the initial perturbation is somewhat of a misleading idea as it is reasonable
to take the zero vector to be the initial perturbation and begin arnoldi iteration
with the vector $-F(u) = b$, such that the Krylov subspace being generated through
GMRES is $K_n = <b, Ab, A^2b, ...>$.

Also I think I should be only using GMRES on half of the spectrum and such that there
are fewer variables in memory, and this way the structure of the Fourier coefficients
remains representative of a real valued velocity field when inverse Fourier transforms
are applied. I didn't do this until now because I didn't think it would be necessary
as I didn't see any mention of it in John's code but I think I should have known better
as this is what I have done in the variational codes at least.

Going to move on to preconditioners next, and I added (I believe up to the standards
of a decent human being modifying the bib file ) two more papers, \refref{Gijzen95,ReSaWo10} in this effort.
I am hoping to bridge the gap between these two with a hybrid method that uses the convenience of matrix-vector
products, the easily formed \jacobianM\ corresponding to the linear portion of the spatiotemporal mapping, and
preconditioning.

Rewrote functions in Newton-Krylov code to run with right-preconditioning
$J M (M^{-1} \delta \conf) = -F(x)$ with a Jacobi (approximate inverse of
\jacobianM\ based on diagonal elements) as a preconditioner, using the
hybrid finite-difference and explicit linear \jacobianM\ method I am trying and mentioned
in yesterday's blog post. Still have more
testing to do and the Jacobi preconditioner is usually not the best preconditioner from what
I have seen. Another common precondition uses the incomplete LU factorization
which I think I will try if it doesn't work.
Still have a couple more things to do before I can test properly but given
the prior difficulties I don't see things magically working out.

More work on torus code; After implementing my idea of defining
the nonlinear \jacobianM\ via finite-differences and the linear \jacobianM\
explicitly didn't work too well, I moved on to trying to see if doing everything
explicitly with use of SciPy's implementation of GMRES does the trick.

I didn't attempt this before because I was trying to explicit define \jacobianMs\
due to the memory usage but now considering I couldn't get the finite-difference
approximation to work (or maybe it's better to say that I don't think it's suited
for this problem) I decided to try doing everything explicitly, or at least
without using finite-differences

Xiong recommended using a Jacobi preconditioner (taking the inverse of the diagonal
elements of the \jacobianM) but I haven't had good results with this yet. I implemented
an ILU (incomplete LU factorization) preconditioner which approximates the inverse
of the entire \jacobianM, as opposed to only the diagonal elements. This worked better,
with Newton steps that actually reduced the residual and didn't send the period or length
to zero and or negative values. The only problem is that after a few
steps it would then fail to decrease the residual.
 I have a feeling this is an indication I am on the right
track but there are still some bugs to work out.

If this doesn't work I think I will attempt an even simpler problem, i.e.
the two modes problem as per Burak's recommendation; much like how I started
with the R\"ossler system for the variational method.
A quick computation of the condition number of the explicitly defined \jacobianM\ using
singular value decomposition reveals that it's in the tens of millions, which I believe
proves that I am dealing with a highly ill-conditioned system of equations.
By disabling changes to the period and length of the system this is lowered by an
order of magnitude, however, I believe this still implies that the system is highly ill-conditioned
even when trying to find fixed points of the spatiotemporal mapping on a fixed spatiotemporal domain.
Xiong suggested that perhaps I can use the singular values to produce a preconditioner that might help,
I'm looking into this as well as some other papers\rf{Meza95}


Iterative
methods inexactly solve linear systems by making progressively
better approximations to the solution, typically by expanding
the subspace the solution is constrained to. A good example
of this type of search are
Typically this occurs when either the cost
functional or the magnitude of the gradient falls beneath some
predetermined value. There are two main classes of algorithms that we use
which can be modified and tweaked on a case-by-case basis. Both of these
two classes, Krylov subspace methods

One consequence of our \spt\ formulation is that the
dimensionality of our BVP problem is inherently
much higher than the IVP from the time dynamical system
formulation. When memory requirements become too large, we employ variants of the
general minimal residual method (GMRES)\rf{Saad1986}. This method
exchanges \refeq{e-lstsq} with a much smaller least-squares optimization
problem by constraining it to the Krylov subspace whose dimension
conventionally much smaller than the dimension of the {\statesp}. The
intricacies of Krylov subspaces will be avoided here, we refer the reader
to\rf{Trefethen97} for details. The Krylov matrix whose columns span the
Krylov subspace associated with the matrix $A$ and vector $b$ is
$K_n =
\bigl[ \begin{smallmatrix} b, & Ab, & A^2b, & ...,& A^{n-1}b
\end{smallmatrix}
\bigl]$
GMRES is derived by performing Arnoldi iteration
on the Krylov matrix. Arnoldi iteration is the process of applying
Gram-Schmidt orthonormalization to the columns of the Krylov matrix. This
provides us with two matrices $Q_n$ and $H_n$ which obey
\bea \label{e-arnoldirelations}
K_n&=&Q_n R_n\continue
H_n&=&Q_n^* A Q_n \continue
A Q_n&=&Q_{n+1}H_n \,.
\eea
The most important relation is the last line of \refeq{e-arnoldirelations} as
substituting this expression into \refeq{e-lstsq} is what leads the GMRES equation.
Explicitly
\bea
\label{eqn:GMRES}
||Ax-b|| &=& ||A Q_k y-b||
     =  ||Q_{k+1} H_k y-b||
     =  ||H_k y-\transp{Q}_{k+1}\,b|| \continue
    &=& ||H_k y-\beta e_1|| \quad \mbox{where } \beta = ||b||
\,,
\eea
such that we have exchanged the problem of solving the full dimensional
problem of \refeq{e-lstsq} with the problem with much smaller dimensionality
\beq \label{e-lstsqGMRES}
\min_{y}  ||H_k y-\beta e_1||\,.
\eeq
This problem can be solved in a number of methods, typically it is solved
via Givens rotations and back substitution, for our purposes we find
it adequate to solve it using the same LAPACK routine used to solve the original equation
directly.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%The right preconditioned arnoldi iteration (with modified Graham-Schmidt for orthogonalization)
%is written as follows in pseudo code.

%for $j = 1, \cdots, m$
%\bea
%z_{j+1} &=& P^{-1} \cdot q_j \continue
%q_{j+1} &=& J \cdot z_{j+1}
%\eea
%for $i = 1, \cdots j$
%\bea
%H_{i,j} &=& <w,q_i> \continue
%q_j &=& q_j - H_{i,j}*q_i
%\eea
%end for
%\bea
%H_{i+1,i} &=& ||q_j||_2 \continue
%q_j &=& q_j/||q_j||_2
%\eea
%end for
There is a very important point of order that must be addressed for those interested
in order to practically employ these methods, namely, how we deal with computational
memory requirements. By choosing to formulate the problem as a \spt\ variational
problem we increase the computational encumbrance because of the increased dimensionality
of the system. In order to increase computation speed and decrease computer memory requirements
we develop matrix-free methods that eliminate and need to explicitly construct any and
all matrices that arise. More detail of this can be found in \refsect{sect:matrixfree}.


{\bf Matt 2017-04-10} ``the effect of changes
to the period and spatial size in expressions such as $F(u + \delta u , T +
\delta T, L + \delta L)$: [...]  why does it make
sense to take difference between two solutions where I am essentially
changing the domain size while keeping the discretization the same?''

{\bf Predrag} I do not think of it as subtracting domains of different sizes:
as far as the discretization is concerned, it is always the same $(N,M)$
discrete lattice, only the values of the field $u(x,t)$ on the lattice sites
are changing as you vary parameters $(L,\period{})$.

Look at the Fourier-discretized torus \refeq{e-FksSpattemp}. The
discretizations $(N,M)$ are the numbers of spatial, temporal Fourier modes
kept in the calculation. Clearly you can keep  $(N,M)$ constant while you
vary $(L,\period{})$ to $(L',\period{}')=(L+\delta L,\period{}+\delta\period{})$.
Spatiotemporal domain size $(L,\period{})$ enters only through the
parametrization
\(
q_k = {2 \pi k}/{L}
    \,,\;
\omega_\ell = {2 \pi \ell}/{\period{}}
\,.
\)





During this process I find it rather confusing and intriguing that the code I have written that
compute hooksteps minimizing the quadratic residual $|A dx - b |^2$ with the constraint $|dx|^2 \leq \delta^2$
where delta is some trust region \emph{seems to only work in physical space for me}. The main culprit is that
when trying to minimize the quadratic form $|A dx - b |^2$ we rewrite this to exploit the fact that
GMRES calculations have already been performed; Namely using the arnoldi iteration recursion relation between
the matrices

\beq \nonumber
A Q_n = Q_{n+1} H_n \, \mbox{where} \,
\eeq

$Q_k$ refers to a matrix whose columns span the $k$th Krylov subspace,

to rewrite the quadratic residual as,
\beq \nonumber
|H_n s - b|^2 \, \mbox{where} \,,
\eeq
$dx = Q_n s$. Performing a singular value decomposition on $H_n$ allows us to simplify the problem such that
linear system becomes diagonal. i.e. with $H_n = U D \transp{V}$, and performing some algebra, we are left with
\beq \nonumber
|D \hat{s} - \hat{b}|^2
\eeq

In the above equation $\hat{s} = \transp{V} s $ and
$\hat{b} = \transp{U} Q_{n+1}^{\top} b$, with $\hat{s}$ being
constrained to be within a trust region described by delta. $|\hat{s}|^2 \leq
\delta^2$. The main way that this problem is tackled by Dennis and Schnable
\rf{Dennis96} is to assume the dependence on a new parameter $\mu$ such that
\[
\hat{s}_i = \frac{\hat{b}_i}{d_i + \mu}
\,,
\]
and then use a modified Newton method to find zeros of the function
\[
\Phi (\mu) = |\hat{s}(\mu)|^2 - \delta^2
\,.
\]

The reason why they use a modified Newton (one that seems to be very specific to the problem) is that
due to the form of the presumed dependence on $\mu$ the regular Newton is found to be suboptimal in
the sense that it always undershoots when making correction. The form of the modified Newton to optimize
this $\mu$ is the following

\beq
\Delta \mu = \frac{|\hat{s}(\mu)| \Phi (\mu)}{\delta \prime{\Phi}(\mu)}
\eeq

which can be seen that the modification is in the form of the prefactor $\hat{s} / \delta$
Testing more stringent conditions to hopefully produce better initial conditions for spatiotemporal
code, they're crude bounds such that during the close recurrence searching procedure there is now a maximum
allowed value for the $L_2$ norm, and after a Poincar\'e intersection has been obtained there is now
a trial to compute what the $L_2$ norm would be for the resultant spatiotemporal mapping. This is helping
produce some better initial conditions which will be passed to the hybrid adjoint Newton-Krylov code I have
currently. I'm also trying to be less brash by choosing a value for the initial domain size that is closer
to the $L=22$ domain size, currently I am trying to find \ppo s for a $L=24$ domain size.


In between testing these new initial conditions I have decided to rewrite
and add some different ways that the hookstep is calculated. This is due
to there being a lack of safety nets for when the hookstep code
previously would fail and the code would have to abort. This seems to be
due to trying to optimize a hookstep for too large of a trust region.
While normally you would just check the residual and then if it isn't
reduced, one would reduce the trust region and continue. But, in my case
this would lead to numerical overflow and the code would abort.
Therefore, a safer, more constrained optimization protocol had to be
adopted. Now, the parameter that computes the size of the corrections is
always bounded and if it fails to lie in these bounds, a value is chosen
based on the iteration.

In other words when asked to find a vector $\hat{s}$ that minimizes $||D
\hat{s} - \hat{b} ||_2 $ subject to $\hat{s} \leq \delta $, the general
solution is given by\rf{Dennis96,ChanKers13},
\beq \label{eqn:hookstep}
\hat{s} = \frac{\hat{b} d_i}{d_i^2 + \mu} \mbox{    (no summation)}
\,,
\eeq
\ie, this is the solution that minimizes $\Phi(\mu) = ||s(\mu)||^2 - \delta^2$.

Now I will elect to follow the procedure in \refref{Dennis96} that instills bounds on the parameter $\mu$.
The lower bound will be chosen by the ``normal"
Newton procedure instead of the one that is adapted to solve for $\mu$ for this problem.

During each iteration to update $\mu$, we calculate lower($\ell$) and upper (u) bounds by

Initializing them at first by $\ell_0 =- \frac{\Phi(0)}{\Phi^{\prime}(0)}$ and $u_0= |\nabla F(x_c)|/\delta_c$.

Then at each step for the lower bound we increase the lower bound by taking the maximum between

\beq
\ell_{+}^N = \mu_c - \frac{\Phi(\mu_c)}{\Phi^{\prime}(\mu_c)}
\eeq

and the current lower bound.

To update the upper bound we decrease it by taking the minimum between the current upper bound and the
current value of mu,
\beq
u_{+} = min(u_c , \mu_c)
\eeq

where $+$ indicates the next value of the parameters, and ``c" indicates the current value (following
\rf{Dennis96} notation as to not confuse myself).

The next iterate $\mu_{+}$ is calculated by the 'adapted' Newton method.

\beq
\mu_{+} = \mu_c - \frac{s(\mu_c)}{\delta_c} \frac{\Phi(\mu_c)}{\Phi^{\prime}(\mu_c)}
\eeq

and then tested to see if it lies in our trusted region $[\ell_{+}, u_{+}]$. If it doesn't,
then the next iterate $\mu_{+}$ is taken to be $\max ((\ell_{+} u_{+})^{1/2}, 10^{-3} u_{+})$
which Dennis and Schnable claim is most often used to calculate the initial value $\mu_0$.

We then let the iteration run until the norm of the hookstep lies in our trust region. (I might change
this to be until the residual of the function $\Phi (\mu)$ is minimized sufficiently).

After these changes, we then need to decide to do with the trust region and our current hookstep.
If the hookstep does not minimize the residual sufficiently when compared to the linear model,
we need to decrease the trust region radius (which they say they just do by halving it but also give
details on calculating a multiplier between one tenth and one half). If the residual compares
favorably to the quadratic residual then we can keep this as a backup step before increasing the
trust region radius by a factor of two and attempting to try again.

Almost done with the automation procedure to find spatiotemporal fixed points associated
with \ppo s. I combined the initial condition generation code and the solution converging code
into one Python script. What it will do is work through
different domain sizes one-by-one, produce initial conditions for the convergence code, and if
the initial conditions satisfy certain requirements on the residual of the cost functional, run
the convergence code on it. Currently have it setup so that long trawls of {\statesp} are performed
as opposed to shorter more accurate trawls, getting some weird results so it could be the step size
is too large, however.

Fixed the bugs regarding trust region being too large; in which case the optimization procedure
would fail or give nonsensical results because the parameter involved would diverge towards negative
infinity. In other words, it seems to work when the approximate solution first evaluted
at $\mu = 0$ in \refeq{eqn:hookstep} is larger than the trust region. I.e. it works when
$|\hat{s}(\mu = 0)| > \delta$. Then, by the effect of increasing $\mu$ ensures that the solution's norm
is being scaled back to within the trust region. If the norm of $s(\mu = 0) < \delta$ then the optimization
tries to enlarge the approximate solution by making the denominator smaller via cancellation. I find no
mention of the in \refref{Dennis96} but they do have a chapter on rescaling. The reason why this is so confusing
to me is that I thought by using my reformulated, rescaled equations I would have already dealt with any scaling
issues but it seems it isn't so easy. I'm going to test if whether the original formulation shares this property
or not, meaning that it could be the case that the original equations are more useful in using the hookstep
method and the rescaled equations are better for descent and regular Newton methods. I find this hard to be
the case because I know a properly scaled matrix is almost required for GMRES to be useful, as it includes
power iteration of matrix vector products so maybe there is still yet another problem with the hookstep code
that hasn't been found as of yet.

I might elect to do a different residual criterion that just uses norms, right now it calculates
a time integration series, and its reflection. Then the close recurrence criterion is based on the
$L_2$ norm between the reflected series and the physical one; the rational being that if we pass
through one prime period of a \ppo, then the endpoint should be the reflected partner
of the initial point. The problem with this is for every time $t$ the $L_2$ norm of the difference
with every reflected future point must be computed. A more efficient solution might be to just compute
all of the $L_2$ norms once and then trust that if an initial point has a similar norm of a reflected
endpoint, then they are symmetric under reflection. This seems hopeful, and I feel like I'm almost
being academically dishonest at this point because this is yet another idea from L{\'o}pez \etal\rf{lop05rel}.


I believe the main source of error (due to the fact that I am using the same exact gmres code)
for certain test cases is that the matrix-vector approximation is just too inaccurate to
produce a reliable Krylov subspace to find solutions in. I'm attempting to work around this with
higher order finite difference approximations for the matrix-vector approximations, but the difference
upon the first iteration between the first order ($\mathcal{O}(\Delta x)^2$ error) and the higher
order ($\mathcal{O}(\Delta x)^4$ error) is approximately $10^-10$. I need to compare the finite difference
approximation to the explicit matrix computation still. 
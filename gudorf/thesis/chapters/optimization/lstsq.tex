% siminos/gudorf/thesis/chapter/gaussnewton.tex
% $Author: predrag $ $Date: 2020-10-24 01:45:26 -0400 (Sat, 24 Oct 2020) $


Technically this equation is solved iteratively, each time producing its own least-squares solution which guides the field to \twot.
The equations are augmented to include variations in $\speriod{},\period{}$ and as
such the linear system is actually rectangular.
We chose to solve the equations in a least-squares manner as we are not focused on finding a unique
solution; any member of a \twots\ group orbit will do. The price of this indefiniteness is that we might
collect \twots\ which belong to the same group orbit. To improve the convergence rate of the algorithm we also include backtracking: the length of the Newton step is reduced until either a minimum length is reached (failure) or the cost function decreases.

The second method is application of a least-squares solver
to the root finding problem $F=0$. The Newton system is
derived here for context.
\beq
\mathbf{f}(\statev+\delta\statev)\approx
\goveqn\ + \nabla\goveqn\ \delta\statev\ + \mathcal{O}(\delta\statev^2)\,.
\eeq
substitution of zero for the LHS (the root) yields
\beq \label{e-newton}
\nabla\goveqn\delta\statev = -\goveqn \,.
\eeq
where $\nabla\goveqn$ is given by \refeq{e-fksjacobian} and its
symmetry variants. The equation \refeq{e-newton} will be referred to as the \textit{Newton equations} of \refeq{e-fks}. The newton equations can be solved by either iterative or direct methods.

solve linear systems $\mathbf{A}\mathbf{x} = \mathbf{b}$ by creating the inverse (or
pseudo-inverse) of $\mathbf{A}$. The underdetermined nature of the equations Because the period and
spatial domain size are maintained as variables
the linear system of equations is represented by a rectangular matrix,
which can be solved by either applying a least-squares solver
or forming the \textit{normal equations} which creates a square
linear system by multiplying by the transpose of the rectangular matrix.

As $A$ in our case is already ill-conditioned because of the stiff linear components;
that is, the higher order spatial derivatives dominate the spectrum of eigenvalues
by manifesting as diagonal matrices with large magnitudes when compared to the typical
matrix element magnitude. Because of this, the product $A^{T}A$ would be affected in turn.
The linear system in question is the Newton system derived from the linear expansion of the
root finding problem $F=0$.

\beq
\text{minimize} ||Ax - b||
\ee{lstsq}
    %\PC{2019-04-17}{where is \refeq{e-linearminimization}?}
In practice \refeq{lstsq} is solved iteratively by constructing the Moore-Penrose
pseudo-inverse, which provides the least-squares solution to \refeq{lstsq} of minimum
norm.
\beq \label{e-lstsqiteration}
\min_{\Delta \mathbf{z}_{i}}  ||DG(\mathbf{z}_{i-1})\Delta \mathbf{z}_{i} +\mathbf{G}(\mathbf{z}_{i-1})||_2
\eeq
for $i = 1,2,3,...$.
This produces a sequence of approximate solutions
$\{\mathbf{z}_i = \mathbf{z}_{i-1}+\Delta \mathbf{z}_{i-1} \}$
which we require to monotonically decrease the value of the cost
functional \refeq{e-costfunctional}. If this monotonic behavior is
violated then we deem the trial a failure. Some common ways of reducing
the likelihood of failure include introducing a line-search,
trust-region, and backtracking techniques\rf{Dennis96}. We elect to
introduce a scalar ``damping'' parameter $\rho$ which modifies the
magnitude of each step produced by any of our iterative methods. The
recursive equation for the sequence of approximate solutions to
\refeq{e-lstsqiteration}
    %\PC{2019-04-17}{where is \refeq{e-}?}
\beq \label{e-dampednewtonstep}
\mathbf{z}_i = \mathbf{z}_{i-1} + \rho_{i-1}^{n} \Delta \mathbf{z}_{i-1}
\eeq
where
\beq \label{e-dampingparameter}
\rho_i^{n} = \frac{1}{2^n}
\eeq
for $n \in \mathbb{Z}^+$. In practice, if $F(\mathbf{z}_i)>F(\mathbf{z}_{i-1})$ then
$n$ is increased until either the cost functional's value decreases $F(\mathbf{z}_i)<F(\mathbf{z}_{i-i}$,
or $\rho_i^n = {1}/{2^8}$.

This proceeds until either the cost functional is within double floating
point precision of zero or the magnitude $|z_i|$ becomes too small.
When computationally viable we elect to use a direct method to solve
\refeq{e-newtonequation}. This entails using a subroutine
which computes the pseudo-inverse of the matrix $DG$. This process
has been optimized by the creators of LAPACK, a Fortran package.
Specifically we use a SciPy's implementation of the linear least
squares solver (LSTSQ) which uses the GELSD driver. This driver solves
the linear least squares problem via a divide and conquer
singular value decomposition (SVD) process to compute
the pseudoinverse.

%Global convergence properties
This method of solving the least-squares problem established by
construction of the Newton system\rf{BoyVan04,Visw08,Visw07b}
